"""
å¤æ‚é£é™©åˆ†æå¼•æ“ - åŒ…å«çœŸæ­£éœ€è¦è®¡ç®—æ—¶é—´çš„ç®—æ³•
åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€å›¾ç½‘ç»œåˆ†æã€æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ç­‰
"""

import numpy as np
import time
import json
from datetime import datetime
from collections import defaultdict, deque
import math
import random
from typing import Dict, List, Tuple
import argparse

class AnalysisProgressTracker:
    """åˆ†æè¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.progress_file = "analysis_progress.json"
        self.start_time = time.time()
        self.current_step = ""
        self.steps_completed = []
        self.total_steps = 10  # é¢„è®¡æ€»æ­¥éª¤æ•°
    
    def update_progress(self, step_name: str, progress: float = None):
        """æ›´æ–°åˆ†æè¿›åº¦"""
        self.current_step = step_name
        if progress is None:
            progress = len(self.steps_completed) / self.total_steps * 100
        
        progress_data = {
            "status": "running",
            "current_step": step_name,
            "steps": self.steps_completed + [step_name],
            "progress_percentage": min(progress, 100),
            "elapsed_time": time.time() - self.start_time
        }
        
        # å†™å…¥è¿›åº¦æ–‡ä»¶
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
        
        print(f"[{datetime.now().strftime('%H:%M:%S')}] {step_name} (è¿›åº¦: {progress:.1f}%)")
    
    def complete_step(self, step_name: str):
        """å®Œæˆä¸€ä¸ªæ­¥éª¤"""
        if step_name not in self.steps_completed:
            self.steps_completed.append(step_name)
        
        progress = len(self.steps_completed) / self.total_steps * 100
        self.update_progress(f"å·²å®Œæˆ: {step_name}", progress)
    
    def finish(self):
        """å®Œæˆæ‰€æœ‰åˆ†æ"""
        progress_data = {
            "status": "completed",
            "current_step": "åˆ†æå®Œæˆ",
            "steps": self.steps_completed,
            "progress_percentage": 100,
            "elapsed_time": time.time() - self.start_time
        }
        
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
        
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ‰ åˆ†æå®Œæˆ! æ€»è€—æ—¶: {time.time() - self.start_time:.2f}ç§’")

# å…¨å±€è¿›åº¦è·Ÿè¸ªå™¨
progress_tracker = AnalysisProgressTracker()

class NetworkAnalyzer:
    """äº¤æ˜“ç½‘ç»œåˆ†æå™¨ - éœ€è¦å¤§é‡è®¡ç®—çš„å›¾ç®—æ³•"""
    
    def __init__(self):
        self.transaction_graph = defaultdict(list)
        self.account_graph = defaultdict(set)
        self.merchant_graph = defaultdict(set)
        
    def build_transaction_network(self, transactions: List[Dict]) -> Dict:
        """æ„å»ºäº¤æ˜“ç½‘ç»œå›¾ - è®¡ç®—å¯†é›†å‹æ“ä½œ"""
        progress_tracker.update_progress("æ„å»ºäº¤æ˜“ç½‘ç»œå›¾ç»“æ„...")
        start_time = time.time()
        
        # æ„å»ºè´¦æˆ·-å•†æˆ·å…³ç³»å›¾
        for i, txn in enumerate(transactions):
            if i % 500 == 0:  # æ›´é¢‘ç¹çš„è¿›åº¦æ›´æ–°
                progress = (i / len(transactions)) * 20  # ç½‘ç»œæ„å»ºå æ€»è¿›åº¦çš„20%
                progress_tracker.update_progress(f"å¤„ç†äº¤æ˜“æ•°æ®: {i}/{len(transactions)}", progress)
                
            account = txn["account_id"]
            merchant = txn["merchant_id"]
            amount = txn["amount"]
            timestamp = datetime.fromisoformat(txn["timestamp"])
            
            # æ·»åŠ è¾¹æƒé‡ï¼ˆåŸºäºäº¤æ˜“é¢‘ç‡å’Œé‡‘é¢ï¼‰
            edge_weight = math.log(1 + amount) * (1 / max(1, len(self.transaction_graph[account])))
            
            self.transaction_graph[account].append({
                "merchant": merchant,
                "weight": edge_weight,
                "timestamp": timestamp,
                "amount": amount
            })
            
            self.account_graph[account].add(merchant)
            self.merchant_graph[merchant].add(account)
        
        progress_tracker.complete_step("äº¤æ˜“ç½‘ç»œå›¾æ„å»ºå®Œæˆ")
        
        # è®¡ç®—ç½‘ç»œç»Ÿè®¡
        progress_tracker.update_progress("è®¡ç®—ç½‘ç»œä¸­å¿ƒæ€§æŒ‡æ ‡...", 25)
        centrality_scores = self.calculate_centrality_measures()
        progress_tracker.complete_step("ä¸­å¿ƒæ€§æŒ‡æ ‡è®¡ç®—å®Œæˆ")
        
        # æ£€æµ‹ç¤¾ç¾¤
        progress_tracker.update_progress("æ£€æµ‹äº¤æ˜“ç¤¾ç¾¤ç»“æ„...", 35)
        communities = self.detect_communities()
        progress_tracker.complete_step("ç¤¾ç¾¤æ£€æµ‹å®Œæˆ")
        
        # å¼‚å¸¸è¿æ¥æ£€æµ‹
        progress_tracker.update_progress("æ£€æµ‹å¼‚å¸¸è¿æ¥æ¨¡å¼...", 40)
        anomalous_patterns = self.detect_anomalous_patterns()
        progress_tracker.complete_step("å¼‚å¸¸æ¨¡å¼æ£€æµ‹å®Œæˆ")
        
        elapsed = time.time() - start_time
        print(f"  ç½‘ç»œåˆ†æå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
        
        return {
            "network_stats": {
                "total_accounts": len(self.account_graph),
                "total_merchants": len(self.merchant_graph),
                "total_edges": sum(len(edges) for edges in self.transaction_graph.values()),
                "avg_degree": np.mean([len(neighbors) for neighbors in self.account_graph.values()]),
                "network_density": self.calculate_network_density()
            },
            "centrality_scores": centrality_scores,
            "communities": communities,
            "anomalous_patterns": anomalous_patterns,
            "processing_time": elapsed
        }
        
    def calculate_centrality_measures(self) -> Dict:
        """è®¡ç®—å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡ - è®¡ç®—å¤æ‚åº¦O(NÂ²)"""
        centrality = {}
        
        # åº¦ä¸­å¿ƒæ€§
        for account in self.account_graph:
            degree = len(self.account_graph[account])
            centrality[account] = {"degree": degree}
            
        # ç®€åŒ–çš„æ¥è¿‘ä¸­å¿ƒæ€§è®¡ç®—
        print("   è®¡ç®—æ¥è¿‘ä¸­å¿ƒæ€§...")
        for i, account in enumerate(self.account_graph):
            if i % 500 == 0:
                print(f"      è¿›åº¦: {i/len(self.account_graph)*100:.1f}%")
                
            # BFSè®¡ç®—æœ€çŸ­è·¯å¾„
            distances = self.bfs_shortest_paths(account)
            avg_distance = np.mean(list(distances.values())) if distances else 0
            closeness = 1 / (1 + avg_distance)
            centrality[account]["closeness"] = closeness
            
        # ä»‹æ•°ä¸­å¿ƒæ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        print("   è®¡ç®—ä»‹æ•°ä¸­å¿ƒæ€§...")
        betweenness = self.calculate_betweenness_centrality()
        for account in centrality:
            centrality[account]["betweenness"] = betweenness.get(account, 0)
            
        return centrality
        
    def bfs_shortest_paths(self, start_account: str, max_distance: int = 3) -> Dict[str, int]:
        """BFSè®¡ç®—æœ€çŸ­è·¯å¾„"""
        distances = {start_account: 0}
        queue = deque([(start_account, 0)])
        
        while queue:
            current, dist = queue.popleft()
            if dist >= max_distance:
                continue
                
            for neighbor in self.account_graph.get(current, []):
                if neighbor not in distances:
                    distances[neighbor] = dist + 1
                    queue.append((neighbor, dist + 1))
                    
        return distances
        
    def calculate_betweenness_centrality(self) -> Dict[str, float]:
        """è®¡ç®—ä»‹æ•°ä¸­å¿ƒæ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        betweenness = defaultdict(float)
        accounts = list(self.account_graph.keys())
        
        # é‡‡æ ·è®¡ç®—ä»¥å‡å°‘è®¡ç®—é‡
        sample_size = min(500, len(accounts))
        sampled_accounts = random.sample(accounts, sample_size)
        
        for i, source in enumerate(sampled_accounts):
            if i % 50 == 0:
                print(f"      ä»‹æ•°ä¸­å¿ƒæ€§è¿›åº¦: {i/len(sampled_accounts)*100:.1f}%")
                
            paths = self.find_shortest_paths_from_source(source)
            for path in paths:
                for node in path[1:-1]:  # ä¸­é—´èŠ‚ç‚¹
                    betweenness[node] += 1.0 / len(paths)
                    
        return dict(betweenness)
        
    def find_shortest_paths_from_source(self, source: str, max_paths: int = 100) -> List[List[str]]:
        """ä»æºèŠ‚ç‚¹æŸ¥æ‰¾æœ€çŸ­è·¯å¾„"""
        paths = []
        visited = set()
        
        def dfs(current, path, depth):
            if len(paths) >= max_paths or depth > 4:
                return
                
            if current in visited and len(path) > 1:
                paths.append(path.copy())
                return
                
            visited.add(current)
            for neighbor in self.account_graph.get(current, []):
                if neighbor not in path:  # é¿å…å¾ªç¯
                    path.append(neighbor)
                    dfs(neighbor, path, depth + 1)
                    path.pop()
            visited.remove(current)
        
        dfs(source, [source], 0)
        return paths
        
    def detect_communities(self) -> List[Dict]:
        """ç¤¾ç¾¤æ£€æµ‹ç®—æ³•"""
        communities = []
        visited = set()
        
        for account in self.account_graph:
            if account not in visited:
                community = self.expand_community(account, visited)
                if len(community) >= 3:  # è‡³å°‘3ä¸ªæˆå‘˜æ‰ç®—ç¤¾ç¾¤
                    communities.append({
                        "members": list(community),
                        "size": len(community),
                        "cohesion": self.calculate_community_cohesion(community)
                    })
                    
        return sorted(communities, key=lambda x: x["size"], reverse=True)
        
    def expand_community(self, seed: str, visited: set) -> set:
        """æ‰©å±•ç¤¾ç¾¤"""
        community = {seed}
        visited.add(seed)
        queue = deque([seed])
        
        while queue:
            current = queue.popleft()
            neighbors = self.account_graph.get(current, set())
            
            for neighbor in neighbors:
                if neighbor not in visited:
                    # è®¡ç®—è¿æ¥å¼ºåº¦
                    connection_strength = self.calculate_connection_strength(current, neighbor)
                    if connection_strength > 0.3:  # é˜ˆå€¼
                        community.add(neighbor)
                        visited.add(neighbor)
                        queue.append(neighbor)
                        
        return community
        
    def calculate_connection_strength(self, account1: str, account2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªè´¦æˆ·ä¹‹é—´çš„è¿æ¥å¼ºåº¦"""
        merchants1 = self.account_graph.get(account1, set())
        merchants2 = self.account_graph.get(account2, set())
        
        if not merchants1 or not merchants2:
            return 0.0
            
        intersection = len(merchants1 & merchants2)
        union = len(merchants1 | merchants2)
        
        return intersection / union if union > 0 else 0.0
        
    def calculate_community_cohesion(self, community: set) -> float:
        """è®¡ç®—ç¤¾ç¾¤å†…èšæ€§"""
        if len(community) < 2:
            return 0.0
            
        total_connections = 0
        possible_connections = len(community) * (len(community) - 1)
        
        for member1 in community:
            for member2 in community:
                if member1 != member2:
                    strength = self.calculate_connection_strength(member1, member2)
                    if strength > 0.1:
                        total_connections += strength
                        
        return total_connections / possible_connections if possible_connections > 0 else 0.0
        
    def detect_anomalous_patterns(self) -> List[Dict]:
        """æ£€æµ‹å¼‚å¸¸äº¤æ˜“æ¨¡å¼"""
        anomalies = []
        
        # æ£€æµ‹å¼‚å¸¸é«˜é¢‘äº¤æ˜“
        for account, transactions in self.transaction_graph.items():
            if len(transactions) > 50:  # é«˜é¢‘äº¤æ˜“è´¦æˆ·
                time_intervals = []
                sorted_txns = sorted(transactions, key=lambda x: x["timestamp"])
                
                for i in range(1, len(sorted_txns)):
                    interval = (sorted_txns[i]["timestamp"] - sorted_txns[i-1]["timestamp"]).total_seconds()
                    time_intervals.append(interval)
                    
                if time_intervals:
                    avg_interval = np.mean(time_intervals)
                    if avg_interval < 300:  # å¹³å‡é—´éš”å°äº5åˆ†é’Ÿ
                        anomalies.append({
                            "type": "high_frequency_trading",
                            "account": account,
                            "transaction_count": len(transactions),
                            "avg_interval_seconds": avg_interval,
                            "risk_score": 0.8
                        })
        
        # æ£€æµ‹åœ†å½¢äº¤æ˜“ï¼ˆæ´—é’±æ¨¡å¼ï¼‰
        circular_patterns = self.detect_circular_transactions()
        anomalies.extend(circular_patterns)
        
        return anomalies
        
    def detect_circular_transactions(self) -> List[Dict]:
        """æ£€æµ‹åœ†å½¢äº¤æ˜“æ¨¡å¼"""
        circular_patterns = []
        
        for account in list(self.account_graph.keys())[:100]:  # é‡‡æ ·æ£€æµ‹
            cycles = self.find_cycles_from_account(account, max_length=6)
            for cycle in cycles:
                if len(cycle) >= 3:
                    circular_patterns.append({
                        "type": "circular_transaction",
                        "cycle": cycle,
                        "length": len(cycle),
                        "risk_score": min(0.9, 0.4 + len(cycle) * 0.1)
                    })
                    
        return circular_patterns
        
    def find_cycles_from_account(self, start: str, max_length: int = 6) -> List[List[str]]:
        """ä»è´¦æˆ·æŸ¥æ‰¾äº¤æ˜“å¾ªç¯"""
        cycles = []
        
        def dfs_cycles(current, path, visited):
            if len(path) > max_length:
                return
                
            if current == start and len(path) > 2:
                cycles.append(path.copy())
                return
                
            if current in visited:
                return
                
            visited.add(current)
            for neighbor in self.account_graph.get(current, []):
                if neighbor == start or neighbor not in path:
                    path.append(neighbor)
                    dfs_cycles(neighbor, path, visited.copy())
                    path.pop()
        
        dfs_cycles(start, [start], set())
        return cycles[:10]  # é™åˆ¶ç»“æœæ•°é‡
        
    def calculate_network_density(self) -> float:
        """è®¡ç®—ç½‘ç»œå¯†åº¦"""
        total_nodes = len(self.account_graph) + len(self.merchant_graph)
        total_edges = sum(len(edges) for edges in self.transaction_graph.values())
        max_possible_edges = total_nodes * (total_nodes - 1)
        
        return total_edges / max_possible_edges if max_possible_edges > 0 else 0.0

class TimeSeriesAnomalyDetector:
    """æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹å™¨ - CPUå¯†é›†å‹ç®—æ³•"""
    
    def __init__(self, window_size: int = 24):
        self.window_size = window_size
        self.seasonal_patterns = {}
        
    def detect_temporal_anomalies(self, transactions: List[Dict]) -> Dict:
        """æ£€æµ‹æ—¶é—´åºåˆ—å¼‚å¸¸ - éœ€è¦å¤§é‡è®¡ç®—"""
        print(" å¼€å§‹æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹...")
        start_time = time.time()
        
        # æŒ‰å°æ—¶èšåˆäº¤æ˜“æ•°æ®
        print("   èšåˆæ—¶é—´åºåˆ—æ•°æ®...")
        hourly_data = self.aggregate_by_hour(transactions)
        
        # å­£èŠ‚æ€§åˆ†æ
        print("   åˆ†æå­£èŠ‚æ€§æ¨¡å¼...")
        seasonal_analysis = self.analyze_seasonal_patterns(hourly_data)
        
        # å¼‚å¸¸æ£€æµ‹
        print("   æ£€æµ‹å¼‚å¸¸æ—¶é—´ç‚¹...")
        anomalies = self.detect_anomalies_with_isolation_forest(hourly_data)
        
        # è¶‹åŠ¿åˆ†æ
        print("   è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡...")
        trend_analysis = self.analyze_trends(hourly_data)
        
        elapsed = time.time() - start_time
        print(f"  æ—¶é—´åºåˆ—åˆ†æå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
        
        return {
            "hourly_statistics": hourly_data,
            "seasonal_patterns": seasonal_analysis,
            "anomalies": anomalies,
            "trend_analysis": trend_analysis,
            "processing_time": elapsed
        }
        
    def aggregate_by_hour(self, transactions: List[Dict]) -> Dict:
        """æŒ‰å°æ—¶èšåˆäº¤æ˜“æ•°æ®"""
        hourly_stats = defaultdict(lambda: {
            "transaction_count": 0,
            "total_amount": 0.0,
            "unique_accounts": set(),
            "unique_merchants": set(),
            "fraud_count": 0
        })
        
        for txn in transactions:
            timestamp = datetime.fromisoformat(txn["timestamp"])
            hour_key = timestamp.strftime("%Y-%m-%d %H")
            
            stats = hourly_stats[hour_key]
            stats["transaction_count"] += 1
            stats["total_amount"] += txn["amount"]
            stats["unique_accounts"].add(txn["account_id"])
            stats["unique_merchants"].add(txn["merchant_id"])
            
            if txn.get("is_fraud", False):
                stats["fraud_count"] += 1
                
        # è½¬æ¢setä¸ºcount
        processed_data = {}
        for hour, stats in hourly_stats.items():
            processed_data[hour] = {
                "transaction_count": stats["transaction_count"],
                "total_amount": stats["total_amount"],
                "avg_amount": stats["total_amount"] / stats["transaction_count"],
                "unique_accounts": len(stats["unique_accounts"]),
                "unique_merchants": len(stats["unique_merchants"]),
                "fraud_count": stats["fraud_count"],
                "fraud_rate": stats["fraud_count"] / stats["transaction_count"]
            }
            
        return processed_data
        
    def analyze_seasonal_patterns(self, hourly_data: Dict) -> Dict:
        """åˆ†æå­£èŠ‚æ€§æ¨¡å¼"""
        patterns = {
            "hourly": defaultdict(list),
            "daily": defaultdict(list),
            "weekly": defaultdict(list)
        }
        
        for hour_str, data in hourly_data.items():
            dt = datetime.strptime(hour_str, "%Y-%m-%d %H")
            
            # å°æ—¶æ¨¡å¼
            patterns["hourly"][dt.hour].append(data["transaction_count"])
            
            # æ—¥æ¨¡å¼  
            patterns["daily"][dt.day].append(data["transaction_count"])
            
            # å‘¨æ¨¡å¼
            patterns["weekly"][dt.weekday()].append(data["transaction_count"])
            
        # è®¡ç®—ç»Ÿè®¡å€¼
        seasonal_stats = {}
        for pattern_type, pattern_data in patterns.items():
            seasonal_stats[pattern_type] = {}
            for key, values in pattern_data.items():
                seasonal_stats[pattern_type][key] = {
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "min": min(values),
                    "max": max(values),
                    "samples": len(values)
                }
                
        return seasonal_stats
        
    def detect_anomalies_with_isolation_forest(self, hourly_data: Dict) -> List[Dict]:
        """ä½¿ç”¨éš”ç¦»æ£®æ—æ£€æµ‹å¼‚å¸¸ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        anomalies = []
        
        # æå–ç‰¹å¾
        features = []
        timestamps = []
        
        for hour_str, data in hourly_data.items():
            dt = datetime.strptime(hour_str, "%Y-%m-%d %H")
            
            # æ„é€ ç‰¹å¾å‘é‡
            feature_vector = [
                data["transaction_count"],
                data["total_amount"],
                data["avg_amount"], 
                data["unique_accounts"],
                data["unique_merchants"],
                data["fraud_rate"],
                dt.hour,  # æ—¶é—´ç‰¹å¾
                dt.weekday(),
                math.sin(2 * math.pi * dt.hour / 24),  # å‘¨æœŸæ€§ç‰¹å¾
                math.cos(2 * math.pi * dt.hour / 24)
            ]
            
            features.append(feature_vector)
            timestamps.append(hour_str)
            
        if not features:
            return anomalies
            
        # ç®€åŒ–çš„å¼‚å¸¸æ£€æµ‹ç®—æ³•
        features_array = np.array(features)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        means = np.mean(features_array, axis=0)
        stds = np.std(features_array, axis=0)
        stds[stds == 0] = 1  # é¿å…é™¤é›¶
        normalized_features = (features_array - means) / stds
        
        # è®¡ç®—å¼‚å¸¸åˆ†æ•°ï¼ˆåŸºäºé©¬æ°è·ç¦»çš„ç®€åŒ–ç‰ˆæœ¬ï¼‰
        for i, (feature_vector, timestamp) in enumerate(zip(normalized_features, timestamps)):
            # è®¡ç®—åˆ°ä¸­å¿ƒç‚¹çš„è·ç¦»
            distance = np.linalg.norm(feature_vector)
            
            # è®¾å®šé˜ˆå€¼ï¼ˆåŸºäºåˆ†å¸ƒçš„95%åˆ†ä½æ•°ï¼‰
            if i % 100 == 0:  # æ˜¾ç¤ºè¿›åº¦
                print(f"      å¼‚å¸¸æ£€æµ‹è¿›åº¦: {i/len(features)*100:.1f}%")
                
            threshold = np.percentile([np.linalg.norm(fv) for fv in normalized_features], 95)
            
            if distance > threshold:
                anomaly_score = min(1.0, (distance - threshold) / threshold)
                anomalies.append({
                    "timestamp": timestamp,
                    "anomaly_score": anomaly_score,
                    "type": "statistical_anomaly",
                    "features": hourly_data[timestamp],
                    "distance": distance,
                    "threshold": threshold
                })
                
        # æŒ‰å¼‚å¸¸åˆ†æ•°æ’åº
        anomalies.sort(key=lambda x: x["anomaly_score"], reverse=True)
        
        return anomalies[:50]  # è¿”å›å‰50ä¸ªå¼‚å¸¸
        
    def analyze_trends(self, hourly_data: Dict) -> Dict:
        """åˆ†æè¶‹åŠ¿"""
        if len(hourly_data) < 2:
            return {"trend": "insufficient_data"}
            
        # æŒ‰æ—¶é—´æ’åº
        sorted_data = sorted(hourly_data.items(), key=lambda x: x[0])
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        window = min(12, len(sorted_data) // 4)  # 12å°æ—¶æˆ–æ•°æ®é‡çš„1/4
        moving_averages = []
        
        for i in range(len(sorted_data) - window + 1):
            window_data = [data[1]["transaction_count"] for data in sorted_data[i:i+window]]
            moving_averages.append(np.mean(window_data))
            
        # è®¡ç®—è¶‹åŠ¿æ–œç‡
        if len(moving_averages) >= 2:
            x = np.arange(len(moving_averages))
            y = np.array(moving_averages)
            
            # ç®€å•çº¿æ€§å›å½’
            slope = np.sum((x - np.mean(x)) * (y - np.mean(y))) / np.sum((x - np.mean(x))**2)
            
            trend_direction = "increasing" if slope > 0.1 else "decreasing" if slope < -0.1 else "stable"
        else:
            slope = 0
            trend_direction = "stable"
            
        return {
            "trend_direction": trend_direction,
            "slope": slope,
            "moving_averages": moving_averages,
            "volatility": np.std(moving_averages) if moving_averages else 0,
            "data_points": len(sorted_data)
        }

class MachineLearningRiskModel:
    """æœºå™¨å­¦ä¹ é£é™©è¯„ä¼°æ¨¡å‹ - éœ€è¦è®­ç»ƒå’Œæ¨ç†æ—¶é—´"""
    
    def __init__(self):
        self.feature_weights = None
        self.training_time = 0
        
    def train_risk_model(self, transactions: List[Dict]) -> Dict:
        """è®­ç»ƒé£é™©è¯„ä¼°æ¨¡å‹ - è®¡ç®—å¯†é›†å‹"""
        print(" è®­ç»ƒæœºå™¨å­¦ä¹ é£é™©æ¨¡å‹...")
        start_time = time.time()
        
        # ç‰¹å¾å·¥ç¨‹
        print("   æå–ç‰¹å¾...")
        features, labels = self.extract_features_and_labels(transactions)
        
        # ç®€åŒ–çš„æ¢¯åº¦ä¸‹é™è®­ç»ƒ
        print("   è®­ç»ƒæ¨¡å‹ (æ¢¯åº¦ä¸‹é™)...")
        self.feature_weights = self.gradient_descent_training(features, labels)
        
        # æ¨¡å‹è¯„ä¼°
        print("   è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        evaluation_metrics = self.evaluate_model(features, labels)
        
        self.training_time = time.time() - start_time
        print(f"  æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {self.training_time:.2f}ç§’")
        
        return {
            "model_performance": evaluation_metrics,
            "feature_importance": self.calculate_feature_importance(),
            "training_samples": len(transactions),
            "training_time": self.training_time
        }
        
    def extract_features_and_labels(self, transactions: List[Dict]) -> Tuple[np.ndarray, np.ndarray]:
        """ç‰¹å¾å·¥ç¨‹ - æå–å’Œæ„é€ ç‰¹å¾"""
        features = []
        labels = []
        
        print("      ç‰¹å¾æå–è¿›åº¦:")
        for i, txn in enumerate(transactions):
            if i % 1000 == 0:
                print(f"        {i/len(transactions)*100:.1f}%")
                
            # åŸºç¡€ç‰¹å¾
            amount = txn.get("amount", 0)
            hour = datetime.fromisoformat(txn["timestamp"]).hour
            
            # é£é™©ç‰¹å¾å·¥ç¨‹
            feature_vector = [
                math.log(1 + amount),  # å¯¹æ•°é‡‘é¢
                1 if hour < 6 or hour > 22 else 0,  # å¼‚å¸¸æ—¶é—´
                len(txn.get("risk_factors", {})),  # é£é™©å› å­æ•°é‡
                txn.get("calculated_risk_score", 0),  # è®¡ç®—çš„é£é™©åˆ†æ•°
                1 if txn.get("location") == "unknown" else 0,  # æœªçŸ¥ä½ç½®
                1 if "crypto" in txn.get("merchant_id", "") else 0,  # åŠ å¯†è´§å¸
                1 if "casino" in txn.get("merchant_id", "") else 0,  # èµŒåš
                txn.get("account_age_days", 0) / 365.0,  # è´¦æˆ·å¹´é¾„ï¼ˆå¹´ï¼‰
                math.sin(2 * math.pi * hour / 24),  # æ—¶é—´å‘¨æœŸæ€§
                math.cos(2 * math.pi * hour / 24),  # æ—¶é—´å‘¨æœŸæ€§
                
                # å¤æ‚ç‰¹å¾ï¼ˆéœ€è¦è®¡ç®—ï¼‰
                self.calculate_velocity_risk(txn),
                self.calculate_merchant_risk(txn),
                self.calculate_geographic_risk(txn),
                self.calculate_behavioral_deviation(txn, transactions)
            ]
            
            features.append(feature_vector)
            labels.append(1 if txn.get("is_fraud", False) else 0)
            
        return np.array(features), np.array(labels)
        
    def calculate_velocity_risk(self, txn: Dict) -> float:
        """è®¡ç®—äº¤æ˜“é€Ÿåº¦é£é™©"""
        velocity = txn.get("velocity_features", {})
        
        # åŸºäº1å°æ—¶å’Œ24å°æ—¶çš„äº¤æ˜“é¢‘ç‡
        txn_1h = velocity.get("transactions_1h", 0)
        amount_1h = velocity.get("amount_1h", 0)
        
        # é€Ÿåº¦é£é™©è¯„åˆ†
        frequency_risk = min(1.0, txn_1h / 10.0)  # 1å°æ—¶è¶…è¿‡10ç¬”ä¸ºé«˜é£é™©
        amount_risk = min(1.0, amount_1h / 50000.0)  # 1å°æ—¶è¶…è¿‡5ä¸‡ä¸ºé«˜é£é™©
        
        return (frequency_risk + amount_risk) / 2.0
        
    def calculate_merchant_risk(self, txn: Dict) -> float:
        """è®¡ç®—å•†æˆ·é£é™©"""
        category = txn.get("merchant_category", "")
        
        # é«˜é£é™©å•†æˆ·ç±»åˆ«
        high_risk_categories = ["crypto", "casino", "jewelry", "electronics"]
        medium_risk_categories = ["online", "atm_cash"]
        
        if category in high_risk_categories:
            return 0.8
        elif category in medium_risk_categories:
            return 0.4
        else:
            return 0.1
            
    def calculate_geographic_risk(self, txn: Dict) -> float:
        """è®¡ç®—åœ°ç†é£é™©"""
        location = txn.get("location", "")
        
        high_risk_locations = ["unknown", "overseas", "macau"]
        if location in high_risk_locations:
            return 0.7
        else:
            return 0.2
            
    def calculate_behavioral_deviation(self, txn: Dict, all_transactions: List[Dict]) -> float:
        """è®¡ç®—è¡Œä¸ºåå·®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        account_id = txn["account_id"]
        current_amount = txn["amount"]
        
        # æŸ¥æ‰¾åŒä¸€è´¦æˆ·çš„å†å²äº¤æ˜“ï¼ˆé‡‡æ ·ä»¥å‡å°‘è®¡ç®—é‡ï¼‰
        account_txns = [t for t in all_transactions[:1000] if t["account_id"] == account_id]
        
        if len(account_txns) < 2:
            return 0.3  # æ–°è´¦æˆ·é»˜è®¤é£é™©
            
        # è®¡ç®—é‡‘é¢åå·®
        amounts = [t["amount"] for t in account_txns if t["id"] != txn["id"]]
        if amounts:
            avg_amount = np.mean(amounts)
            std_amount = np.std(amounts) if len(amounts) > 1 else avg_amount
            
            # Z-score
            z_score = abs(current_amount - avg_amount) / max(std_amount, 1.0)
            deviation_score = min(1.0, z_score / 3.0)  # 3ä¸ªæ ‡å‡†å·®ä¸ºæ»¡åˆ†
            
            return deviation_score
        else:
            return 0.3
            
    def gradient_descent_training(self, features: np.ndarray, labels: np.ndarray, 
                                epochs: int = 100, learning_rate: float = 0.01) -> np.ndarray:
        """ç®€åŒ–çš„æ¢¯åº¦ä¸‹é™è®­ç»ƒ"""
        n_features = features.shape[1]
        weights = np.random.normal(0, 0.1, n_features)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        feature_means = np.mean(features, axis=0)
        feature_stds = np.std(features, axis=0)
        feature_stds[feature_stds == 0] = 1
        
        normalized_features = (features - feature_means) / feature_stds
        
        for epoch in range(epochs):
            if epoch % 20 == 0:
                print(f"      è®­ç»ƒè¿›åº¦: {epoch/epochs*100:.1f}%")
                
            # å‰å‘ä¼ æ’­
            logits = np.dot(normalized_features, weights)
            predictions = self.sigmoid(logits)
            
            # è®¡ç®—æŸå¤±æ¢¯åº¦
            error = predictions - labels
            gradients = np.dot(normalized_features.T, error) / len(labels)
            
            # æ›´æ–°æƒé‡
            weights -= learning_rate * gradients
            
            # å­¦ä¹ ç‡è¡°å‡
            if epoch > 50:
                learning_rate *= 0.99
                
        return weights
        
    def sigmoid(self, x: np.ndarray) -> np.ndarray:
        """Sigmoidæ¿€æ´»å‡½æ•°"""
        # é˜²æ­¢æº¢å‡º
        x = np.clip(x, -500, 500)
        return 1 / (1 + np.exp(-x))
        
    def evaluate_model(self, features: np.ndarray, labels: np.ndarray) -> Dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if self.feature_weights is None:
            return {"error": "Model not trained"}
            
        # æ ‡å‡†åŒ–ç‰¹å¾
        feature_means = np.mean(features, axis=0)
        feature_stds = np.std(features, axis=0)
        feature_stds[feature_stds == 0] = 1
        normalized_features = (features - feature_means) / feature_stds
        
        # é¢„æµ‹
        logits = np.dot(normalized_features, self.feature_weights)
        probabilities = self.sigmoid(logits)
        predictions = (probabilities > 0.5).astype(int)
        
        # è®¡ç®—æŒ‡æ ‡
        tp = np.sum((predictions == 1) & (labels == 1))
        fp = np.sum((predictions == 1) & (labels == 0))
        tn = np.sum((predictions == 0) & (labels == 0))
        fn = np.sum((predictions == 0) & (labels == 1))
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        accuracy = (tp + tn) / len(labels)
        
        return {
            "accuracy": accuracy,
            "precision": precision, 
            "recall": recall,
            "f1_score": f1_score,
            "confusion_matrix": {
                "tp": int(tp), "fp": int(fp),
                "tn": int(tn), "fn": int(fn)
            }
        }
        
    def calculate_feature_importance(self) -> Dict:
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§"""
        if self.feature_weights is None:
            return {}
            
        feature_names = [
            "log_amount", "unusual_time", "risk_factor_count", "calculated_risk",
            "unknown_location", "crypto_merchant", "casino_merchant", "account_age",
            "time_sin", "time_cos", "velocity_risk", "merchant_risk", 
            "geographic_risk", "behavioral_deviation"
        ]
        
        # å½’ä¸€åŒ–æƒé‡ä½œä¸ºé‡è¦æ€§
        abs_weights = np.abs(self.feature_weights)
        normalized_importance = abs_weights / np.sum(abs_weights)
        
        importance_dict = {}
        for name, importance in zip(feature_names, normalized_importance):
            importance_dict[name] = float(importance)
            
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_importance = dict(sorted(importance_dict.items(), 
                                      key=lambda x: x[1], reverse=True))
        
        return sorted_importance

def run_comprehensive_analysis(dataset_file: str = "complex_transaction_dataset.json") -> Dict:
    """è¿è¡Œå®Œæ•´çš„å¤æ‚åˆ†ææµç¨‹"""
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
    global progress_tracker
    progress_tracker = AnalysisProgressTracker()
    progress_tracker.total_steps = 8  # æ›´æ–°æ€»æ­¥éª¤æ•°
    
    progress_tracker.update_progress("å¼€å§‹ç»¼åˆé£é™©åˆ†æ...", 0)
    print(" å¼€å§‹comprehensive analysis...")
    print("  è¿™å°†éœ€è¦å¤§é‡è®¡ç®—æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print("="*80)
    
    overall_start = time.time()
    
    # åŠ è½½æ•°æ®
    progress_tracker.update_progress("åŠ è½½äº¤æ˜“æ•°æ®é›†...", 5)
    with open(dataset_file, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    transactions = dataset["transactions"]
    progress_tracker.complete_step(f"æ•°æ®é›†åŠ è½½å®Œæˆ: {len(transactions)} ç¬”äº¤æ˜“")
    
    results = {"analysis_modules": {}}
    
    # 1. ç½‘ç»œåˆ†æ
    progress_tracker.update_progress("å¼€å§‹äº¤æ˜“ç½‘ç»œåˆ†æ...", 10)
    network_analyzer = NetworkAnalyzer()
    network_results = network_analyzer.build_transaction_network(transactions)
    results["analysis_modules"]["network_analysis"] = network_results
    progress_tracker.complete_step("ç½‘ç»œåˆ†ææ¨¡å—å®Œæˆ")
    
    # 2. æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹
    progress_tracker.update_progress("å¼€å§‹æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹...", 50)
    time_analyzer = TimeSeriesAnomalyDetector()
    time_results = time_analyzer.detect_temporal_anomalies(transactions)
    results["analysis_modules"]["temporal_analysis"] = time_results
    progress_tracker.complete_step("æ—¶åºåˆ†ææ¨¡å—å®Œæˆ")
    
    # 3. æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ
    progress_tracker.update_progress("å¼€å§‹æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ...", 75)
    ml_model = MachineLearningRiskModel()
    ml_results = ml_model.train_risk_model(transactions)
    results["analysis_modules"]["machine_learning"] = ml_results
    progress_tracker.complete_step("æœºå™¨å­¦ä¹ æ¨¡å—å®Œæˆ")
    
    # ç»¼åˆç»“æœ
    progress_tracker.update_progress("ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...", 90)
    total_time = time.time() - overall_start
    
    results["comprehensive_summary"] = {
        "total_processing_time": total_time,
        "transactions_analyzed": len(transactions),
        "analysis_modules_completed": len(results["analysis_modules"]),
        "performance_metrics": {
            "transactions_per_second": len(transactions) / total_time,
            "network_analysis_time": network_results["processing_time"],
            "temporal_analysis_time": time_results["processing_time"], 
            "ml_training_time": ml_results["training_time"]
        }
    }
    
    progress_tracker.complete_step("ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    progress_tracker.finish()  # å®Œæˆæ‰€æœ‰åˆ†æ
    
    print("ç»¼åˆåˆ†æå®Œæˆ!")
    print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f" å¤„ç†é€Ÿåº¦: {len(transactions)/total_time:.1f} äº¤æ˜“/ç§’")
    print("="*80)
    
    return results

def run_ml_only(dataset_file: str = "complex_transaction_dataset.json") -> Dict:
    """ä»…è¿è¡Œæœºå™¨å­¦ä¹ æ¨¡å—ï¼Œç”Ÿæˆä¸ç»¼åˆåˆ†æç›¸åŒç»“æ„çš„ç»“æœæ–‡ä»¶ã€‚"""
    overall_start = time.time()
    with open(dataset_file, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    transactions = dataset.get("transactions", [])

    # æœºå™¨å­¦ä¹ è®­ç»ƒ
    ml_model = MachineLearningRiskModel()
    ml_results = ml_model.train_risk_model(transactions)

    total_time = time.time() - overall_start
    results = {
        "analysis_modules": {
            "machine_learning": ml_results
        },
        "comprehensive_summary": {
            "total_processing_time": total_time,
            "transactions_analyzed": len(transactions),
            "analysis_modules_completed": 1,
            "performance_metrics": {
                "transactions_per_second": len(transactions) / total_time if total_time > 0 else 0.0,
                "ml_training_time": ml_results.get("training_time", 0.0)
            }
        }
    }
    return results

def run_network_only(dataset_file: str = "complex_transaction_dataset.json") -> Dict:
    overall_start = time.time()
    with open(dataset_file, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    transactions = dataset.get("transactions", [])

    network_analyzer = NetworkAnalyzer()
    network_results = network_analyzer.build_transaction_network(transactions)

    total_time = time.time() - overall_start
    return {
        "analysis_modules": {"network_analysis": network_results},
        "comprehensive_summary": {
            "total_processing_time": total_time,
            "transactions_analyzed": len(transactions),
            "analysis_modules_completed": 1,
            "performance_metrics": {
                "transactions_per_second": len(transactions) / total_time if total_time > 0 else 0.0,
                "network_analysis_time": network_results.get("processing_time", 0.0)
            }
        }
    }

def run_time_only(dataset_file: str = "complex_transaction_dataset.json") -> Dict:
    overall_start = time.time()
    with open(dataset_file, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    transactions = dataset.get("transactions", [])

    time_analyzer = TimeSeriesAnomalyDetector()
    time_results = time_analyzer.detect_temporal_anomalies(transactions)

    total_time = time.time() - overall_start
    return {
        "analysis_modules": {"temporal_analysis": time_results},
        "comprehensive_summary": {
            "total_processing_time": total_time,
            "transactions_analyzed": len(transactions),
            "analysis_modules_completed": 1,
            "performance_metrics": {
                "transactions_per_second": len(transactions) / total_time if total_time > 0 else 0.0,
                "temporal_analysis_time": time_results.get("processing_time", 0.0)
            }
        }
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Complex Analysis Engine")
    parser.add_argument("--mode", choices=["comprehensive", "ml_only", "network_only", "time_only"], default="comprehensive")
    parser.add_argument("--data", default="complex_transaction_dataset.json")
    args = parser.parse_args()

    if args.mode == "ml_only":
        analysis_results = run_ml_only(args.data)
    elif args.mode == "network_only":
        analysis_results = run_network_only(args.data)
    elif args.mode == "time_only":
        analysis_results = run_time_only(args.data)
    else:
        analysis_results = run_comprehensive_analysis(args.data)

    # ä¿å­˜åˆ†æç»“æœ
    with open("comprehensive_analysis_results.json", "w", encoding="utf-8") as f:
        json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
    print(" åˆ†æç»“æœå·²ä¿å­˜ä¸º 'comprehensive_analysis_results.json'")